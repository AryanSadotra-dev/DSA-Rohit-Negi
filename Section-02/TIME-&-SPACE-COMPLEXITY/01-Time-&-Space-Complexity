/*
====================================================================================
üìò LECTURE NOTES
TOPIC: TIME & SPACE COMPLEXITY (COMPLETE MASTER LECTURE)
STYLE: Teacher Classroom Style | English | GitHub Ready | Single File
====================================================================================

‚ö†Ô∏è These notes are a DIRECT reconstruction of the lecture.
‚ö†Ô∏è Logic, flow, examples, and thinking process are preserved.
‚ö†Ô∏è No summarization. No skipping.
‚ö†Ô∏è Focus on interview + coding round understanding.

====================================================================================
1Ô∏è‚É£ INTRODUCTION: WHY TIME & SPACE COMPLEXITY?
====================================================================================

Hello Coder Army, how are you all?

Today we are going to study one of the MOST IMPORTANT topics in DSA:
üëâ TIME and SPACE COMPLEXITY

This topic is asked in:
- Every interview round
- Every coding round
- Every serious DSA discussion

Once you truly understand this topic,
üëâ you will NEVER need to revise it again.

We will:
- Understand the concept deeply
- Practice many questions
- Build intuition instead of memorization

====================================================================================
2Ô∏è‚É£ WHAT IS TIME COMPLEXITY? (INTUITIVE START)
====================================================================================

Question:
What do you think time complexity is?

Common answer:
"Total time taken by an algorithm"

‚úîÔ∏è That is partially correct.

But let‚Äôs step back and ask:
‚ùì WHY do we even need time complexity?

------------------------------------------------------------------------------------
2.1 MULTIPLE SOLUTIONS PROBLEM
------------------------------------------------------------------------------------

For one problem, there can be multiple algorithms:

- A1
- A2
- A3
- A4

Question:
Which algorithm should we choose?

Answer:
üëâ The one that takes LESS TIME.

But here comes the real problem‚Ä¶

====================================================================================
3Ô∏è‚É£ WHY WE CANNOT MEASURE TIME IN SECONDS
====================================================================================

Suppose we measure time in seconds.

Example problem:
Sum of first N numbers.

Algorithm:
for (i = 1 to n)
    sum += i;

Now run this algorithm on:
- Old computer
- New computer (latest MacBook)

For N = 1000:
- Old computer ‚Üí 2 seconds
- New computer ‚Üí 1 second

For N = 100000:
- Old computer ‚Üí 200 seconds
- New computer ‚Üí 100 seconds

Now answer this:
‚ùì What is the time of this algorithm?

- 1 second?
- 2 seconds?
- 100 seconds?
- 200 seconds?

üëâ MULTIPLE ANSWERS ‚ùå

So we conclude:
‚ùå Time in seconds is NOT reliable.

Reasons:
- Hardware difference
- Background processes
- OS scheduling
- CPU, RAM, load, etc.

Even on SAME machine:
- Same input may give 190 sec or 210 sec

üëâ So seconds-based comparison FAILS.

====================================================================================
4Ô∏è‚É£ THE REAL DEPENDENCY: INPUT SIZE (N)
====================================================================================

Key observation:
As input size (N) increases,
execution time also increases.

So time depends on:
üëâ SIZE OF INPUT (N)

Example:
- N = 1000 ‚Üí small time
- N = 100000 ‚Üí large time
- N = 1 crore ‚Üí very large time

Hence:

üìå Time Complexity =
Total time taken by an algorithm
AS A FUNCTION OF INPUT SIZE (N)

------------------------------------------------------------------------------------
Formal Definition:
------------------------------------------------------------------------------------

Time Complexity is the total time taken by an algorithm
to run as a function of the length of the input.

====================================================================================
5Ô∏è‚É£ GRAPH INTUITION (OLD vs NEW COMPUTER)
====================================================================================

Plot:
X-axis ‚Üí Input size (N)
Y-axis ‚Üí Time

Old Computer:
- N = 10 ‚Üí 10 sec
- N = 20 ‚Üí 20 sec
- N = 50 ‚Üí 50 sec
‚Üí Straight line (Linear)

New Computer:
- N = 10 ‚Üí 5 sec
- N = 20 ‚Üí 10 sec
- N = 50 ‚Üí 25 sec
‚Üí Also straight line (Linear)

Important observation:
- Slopes are different
- Pattern is SAME (Linear growth)

So we DO NOT care about:
- Constants (¬Ω, 2, 5, etc.)

We ONLY care about:
üëâ GROWTH PATTERN

====================================================================================
6Ô∏è‚É£ GROWTH CATEGORIES
====================================================================================

Common growth patterns:
- Constant ‚Üí O(1)
- Linear ‚Üí O(n)
- Quadratic ‚Üí O(n¬≤)
- Cubic ‚Üí O(n¬≥)
- Logarithmic ‚Üí O(log n)
- Exponential ‚Üí O(2‚Åø)
- Factorial ‚Üí O(n!)

Algorithms with same growth pattern
belong to SAME category.

====================================================================================
7Ô∏è‚É£ BEST, WORST & AVERAGE CASE
====================================================================================

Three cases for an algorithm:

1Ô∏è‚É£ Best Case ‚Üí Œ© (Omega)
2Ô∏è‚É£ Worst Case ‚Üí O (Big-O)
3Ô∏è‚É£ Average Case ‚Üí Œò (Theta)

------------------------------------------------------------------------------------
Why WORST CASE is most important?
------------------------------------------------------------------------------------

Real-life example:
CBSE website crashes on result day.

Why?
Because it was NOT designed for worst case
(1 lakh students accessing together).

üëâ In real systems, we prepare for WORST CASE.

So in interviews and design:
‚úîÔ∏è We prioritize WORST CASE.

====================================================================================
8Ô∏è‚É£ BIG-O, OMEGA & THETA
====================================================================================

O (Big-O):
- Worst case
- Maximum time taken

Œ© (Omega):
- Best case
- Minimum time taken

Œò (Theta):
- Average / Exact behavior

====================================================================================
9Ô∏è‚É£ RULES FOR CALCULATING TIME COMPLEXITY
====================================================================================

RULE 1Ô∏è‚É£: Ignore constants
- O(3n + 5) ‚Üí O(n)

RULE 2Ô∏è‚É£: Take the dominant term
- O(n¬≤ + n + 10) ‚Üí O(n¬≤)

Why?
Because for large N:
- Smaller terms become negligible

====================================================================================
üîü STEP COUNT METHOD (CORE TECHNIQUE)
====================================================================================

Example:
for (i = 1; i <= n; i++)
    print("Chamaka");

Steps per iteration:
- Comparison
- Print
- Increment

Runs N times ‚Üí O(n)

------------------------------------------------------------------------------------
Key idea:
Count how many TIMES the main work runs.

====================================================================================
1Ô∏è‚É£1Ô∏è‚É£ LINEAR SEARCH ANALYSIS
====================================================================================

Array: [6, 2, 4, 8, 9]

Search 6:
- Found at first index
- Best case ‚Üí Œ©(1)

Search 9:
- Found at last index
- Worst case ‚Üí O(n)

Average case:
- (1 + 2 + ... + n) / n = n/2
- Œò(n)

====================================================================================
1Ô∏è‚É£2Ô∏è‚É£ CONSTANT TIME EXAMPLES
====================================================================================

Example:
print first 10 numbers regardless of N.

Time Complexity:
O(1)

Even if N = 1 million,
still prints 10 numbers only.

====================================================================================
1Ô∏è‚É£3Ô∏è‚É£ MATHEMATICAL FORMULA EXAMPLE
====================================================================================

Sum of N numbers using formula:
n * (n + 1) / 2

No loop.
One calculation.

Time Complexity:
O(1)

====================================================================================
1Ô∏è‚É£4Ô∏è‚É£ NESTED LOOPS (IMPORTANT)
====================================================================================

Case 1:
for i = 1 to n
  for j = 1 to n
    print

‚Üí n * n = n¬≤
‚Üí O(n¬≤)

------------------------------------------------------------------------------------
Case 2:
for i = 1 to n
  for j = 1 to i
    print

Total prints:
1 + 2 + 3 + ... + n
= n(n+1)/2
‚Üí O(n¬≤)

------------------------------------------------------------------------------------
Case 3:
for i = 1 to n
  for j = 1 to i¬≤
    print

Total:
1¬≤ + 2¬≤ + ... + n¬≤
= n(n+1)(2n+1)/6
‚Üí O(n¬≥)

====================================================================================
1Ô∏è‚É£5Ô∏è‚É£ LOGARITHMIC LOOPS
====================================================================================

Example:
i = 1
while (i <= n)
  i = i * 2

Values:
1 ‚Üí 2 ‚Üí 4 ‚Üí 8 ‚Üí ... ‚Üí n

How many steps?
log‚ÇÇ(n)

Time Complexity:
O(log n)

Same applies when:
- Dividing by 2
- Multiplying by 3 ‚Üí log‚ÇÉ(n)

====================================================================================
1Ô∏è‚É£6Ô∏è‚É£ THREE NESTED LOOPS (DEPENDENCY CHECK)
====================================================================================

If inner loops DO NOT depend on outer:
Multiply their counts.

If inner loop depends on outer:
You must SUM the steps.

Dependency check is CRITICAL.

====================================================================================
1Ô∏è‚É£7Ô∏è‚É£ HARMONIC SERIES CASE
====================================================================================

Example:
for i = 1 to n
  for j = 1 to n/i
    print

Total:
n(1 + 1/2 + 1/3 + ... + 1/n)

This is Harmonic Series.

Time Complexity:
O(n log n)

====================================================================================
1Ô∏è‚É£8Ô∏è‚É£ SPACE COMPLEXITY
====================================================================================

Space complexity also depends on INPUT SIZE.

Definition:
Space used by algorithm as a function of input size.

Two types:
1Ô∏è‚É£ Auxiliary Space
2Ô∏è‚É£ Total Space

------------------------------------------------------------------------------------
Auxiliary Space:
Extra memory used (excluding input)

------------------------------------------------------------------------------------
Total Space:
Input space + Auxiliary space

====================================================================================
1Ô∏è‚É£9Ô∏è‚É£ SPACE EXAMPLES
====================================================================================

Example 1:
int a, b, c;

Auxiliary space:
O(1)

------------------------------------------------------------------------------------
Example 2:
Given array of size n
Create another array of size n

Auxiliary space:
O(n)

Total space:
O(n) + O(n) = O(n)

====================================================================================
2Ô∏è‚É£0Ô∏è‚É£ ORDER COMPARISON (IMPORTANT)
====================================================================================

Worst ‚Üí Best:

O(n!)
O(2‚Åø)
O(n¬≥)
O(n¬≤)
O(n log n)
O(n)
O(log n)
O(1)

To compare:
- Plug small values
- Observe growth
- Larger growth dominates in long run

====================================================================================
2Ô∏è‚É£1Ô∏è‚É£ FINAL TAKEAWAYS
====================================================================================

‚úîÔ∏è Never measure time in seconds
‚úîÔ∏è Always relate to input size
‚úîÔ∏è Ignore constants
‚úîÔ∏è Focus on dominant term
‚úîÔ∏è Prepare for worst case
‚úîÔ∏è Learn to detect patterns
‚úîÔ∏è Space matters too

====================================================================================
END OF LECTURE
JAI HIND üáÆüá≥ | JAI BHARAT üáÆüá≥
====================================================================================
*/
